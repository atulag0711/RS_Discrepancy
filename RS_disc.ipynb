{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if th.cuda.is_available():\n",
    "    dtype = th.cuda.DoubleTensor\n",
    "else:\n",
    "    dtype = th.DoubleTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turbulence Neural network\n",
    "class TurbNN(th.nn.Module):\n",
    "\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        Architecture of the turbulence deep neural net\n",
    "        Args:\n",
    "            D_in (Int) = Number of input parameters\n",
    "            H (Int) = Number of hidden paramters\n",
    "            D_out (Int) = Number of output parameters\n",
    "        \"\"\"\n",
    "        super(TurbNN, self).__init__()\n",
    "        self.linear1 = th.nn.Linear(D_in, H)\n",
    "        self.f1 = th.nn.LeakyReLU()\n",
    "        self.linear2 = th.nn.Linear(H, H)\n",
    "        self.f2 = th.nn.LeakyReLU()\n",
    "        self.linear3 = th.nn.Linear(H, H)\n",
    "        self.f3 = th.nn.LeakyReLU()\n",
    "        self.linear4 = th.nn.Linear(H, H)\n",
    "        self.f4 = th.nn.LeakyReLU()\n",
    "        self.linear5 = th.nn.Linear(H, int(H/5))\n",
    "        self.f5 = th.nn.LeakyReLU()\n",
    "        self.linear6 = th.nn.Linear(int(H/5), int(H/10))\n",
    "        self.f6 = th.nn.LeakyReLU()\n",
    "        self.linear7 = th.nn.Linear(int(H/10), D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the neural network\n",
    "        Args:\n",
    "            x (th.DoubleTensor): [N x D_in] column matrix of training inputs\n",
    "        Returns:\n",
    "            out (th.DoubleTensor): [N x D_out] matrix of neural network outputs\n",
    "        \"\"\"\n",
    "        lin1 = self.f1(self.linear1(x))\n",
    "        lin2 = self.f2(self.linear2(lin1))\n",
    "        lin3 = self.f3(self.linear3(lin2))\n",
    "        lin4 = self.f4(self.linear4(lin3))\n",
    "        lin5 = self.f5(self.linear5(lin4))\n",
    "        lin6 = self.f6(self.linear6(lin5))\n",
    "        out = self.linear7(lin6)\n",
    "\n",
    "        return out # these would the the 10 G value which acts acts coeff to the 10 tensor invarients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default tensor type\n",
    "dtype = th.DoubleTensor\n",
    "class Invariant():\n",
    "    \"\"\"\n",
    "    Class used for invariant calculations\n",
    "    \"\"\"\n",
    "    def getInvariants(self, s0, r0):\n",
    "        \"\"\"\n",
    "        Calculates the invariant neural network inputs\n",
    "        Args:\n",
    "            s0 (DoubleTensor): [nCellx3x3] Rate-of-strain tensor -> 0.5*(k/e)(du + du')\n",
    "            r0 (DoubleTensor): [nCellx3x3] Rotational tensor -> 0.5*(k/e)(du - du')\n",
    "        Returns:\n",
    "            invar (DoubleTensor): [nCellx5x1] Tensor containing the 5 invariant NN inputs\n",
    "        \"\"\"\n",
    "        # Invariant Training inputs\n",
    "        # For equations see Eq. 14 in paper\n",
    "        # SB Pope 1975 (http://doi.org/10.1017/S0022112075003382)\n",
    "        # Or Section 11.9.2 (page 453) of Turbulent Flows by SB Pope\n",
    "        nCells = s0.size()[0]\n",
    "        invar = th.DoubleTensor(nCells, 5).type(dtype)\n",
    "\n",
    "        s2 = s0.bmm(s0)\n",
    "        r2 = r0.bmm(r0)\n",
    "        s3 = s2.bmm(s0)\n",
    "        r2s = r2.bmm(s0)\n",
    "        r2s2 = r2.bmm(s2)\n",
    "\n",
    "        invar[:,0] = (s2[:,0,0]+s2[:,1,1]+s2[:,2,2]) #Tr(s2)\n",
    "        invar[:,1] = (r2[:,0,0]+r2[:,1,1]+r2[:,2,2]) #Tr(r2)\n",
    "        invar[:,2] = (s3[:,0,0]+s3[:,1,1]+s3[:,2,2]) #Tr(s3)\n",
    "        invar[:,3] = (r2s[:,0,0]+r2s[:,1,1]+r2s[:,2,2]) #Tr(r2s)\n",
    "        invar[:,4] = (r2s2[:,0,0]+r2s2[:,1,1]+r2s2[:,2,2]) #Tr(r2s2)\n",
    "\n",
    "        # Scale invariants by sigmoid function\n",
    "        # Can use other scalings here\n",
    "        invar_sig = (1.0 - th.exp(-invar))/(1.0 + th.exp(-invar))\n",
    "        invar_sig[invar_sig != invar_sig] = 0\n",
    "        \n",
    "        return invar_sig\n",
    "\n",
    "    def getTensorFunctions(self, s0, r0):\n",
    "        \"\"\"\n",
    "        Calculates the linear independent tensor functions for calculating the\n",
    "        deviatoric  component of the Reynolds stress. Ref: S. Pope 1975 in JFM\n",
    "        Args:\n",
    "            s0 (DoubleTensor): [nCellsx3x3] Rate-of-strain tensor -> 0.5*(k/e)(du + du')\n",
    "            r0 (DoubleTensor): [nCellsx3x3] Rotational tensor -> 0.5*(k/e)(du - du')\n",
    "        Returns:\n",
    "            invar (DoubleTensor): [nCellsx10x[3x3]] Tensor 10 linear independent functions\n",
    "        \"\"\"\n",
    "        # Invariant Training inputs\n",
    "        # For equations see Eq. 15 in paper\n",
    "        # Or SB Pope 1975 (http://doi.org/10.1017/S0022112075003382)\n",
    "        # Or Section 11.9.2 (page 453) of Turbulent Flows by SB Pope\n",
    "        nCells = s0.size()[0]\n",
    "        invar_func = th.DoubleTensor(nCells,10,3,3).type(dtype)\n",
    "\n",
    "        s2 = s0.bmm(s0)\n",
    "        r2 = r0.bmm(r0)\n",
    "        sr = s0.bmm(r0)\n",
    "        rs = r0.bmm(s0)\n",
    "\n",
    "        invar_func[:,0] = s0\n",
    "        invar_func[:,1] = sr - rs\n",
    "        invar_func[:,2] = s2 - (1.0/3.0)*th.eye(3).type(dtype)*(s2[:,0,0]+s2[:,1,1]+s2[:,2,2]).unsqueeze(1).unsqueeze(1)\n",
    "        invar_func[:,3] = r2 - (1.0/3.0)*th.eye(3).type(dtype)*(r2[:,0,0]+r2[:,1,1]+r2[:,2,2]).unsqueeze(1).unsqueeze(1)\n",
    "        invar_func[:,4] = r0.bmm(s2) - s2.bmm(r0)\n",
    "        t0 = s0.bmm(r2)\n",
    "        invar_func[:,5] = r2.bmm(s0) + s0.bmm(r2) - (2.0/3.0)*th.eye(3).type(dtype)*(t0[:,0,0]+t0[:,1,1]+t0[:,2,2]).unsqueeze(1).unsqueeze(1)\n",
    "        invar_func[:,6] = rs.bmm(r2) - r2.bmm(sr)\n",
    "        invar_func[:,7] = sr.bmm(s2) - s2.bmm(rs)\n",
    "        t0 = s2.bmm(r2)\n",
    "        invar_func[:,8] = r2.bmm(s2) + s2.bmm(r2) - (2.0/3.0)*th.eye(3).type(dtype)*(t0[:,0,0]+t0[:,1,1]+t0[:,2,2]).unsqueeze(1).unsqueeze(1)\n",
    "        invar_func[:,9] = r0.bmm(s2).bmm(r2) + r2.bmm(s2).bmm(r0)\n",
    "\n",
    "        # Scale the tensor basis functions by the L2 norm\n",
    "        l2_norm = th.DoubleTensor(invar_func.size(0), 10)\n",
    "        l2_norm = 0\n",
    "        for (i, j), x in np.ndenumerate(np.zeros((3,3))):\n",
    "            l2_norm += th.pow(invar_func[:,:,i,j],2)\n",
    "        invar_func = invar_func/th.sqrt(l2_norm).unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "        return invar_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvarientNN():\n",
    "    \n",
    "    \"\"\"\n",
    "    Class for formulating Bayesian posterior, training and prediction\n",
    "    \"\"\"\n",
    "    def __init__(self,S,R,k,b_avg):\n",
    "        self.S=S # Strain rate tensor\n",
    "        self.R=R # Rotation rate tensor\n",
    "        self.k=k # turbulent kinetic energy\n",
    "        self.b_avg=b_avg # RS term anisotropic tensor\n",
    "        \n",
    "        nData=S.size()[0]\n",
    "        self.x_train = th.Tensor(nData,5).type(dtype) # Invariant inputs\n",
    "        self.t_train = th.Tensor(nData,10,3,3).type(dtype) # Tensor basis\n",
    "        self.k_train = th.Tensor(nData).type(dtype) # RANS TKE\n",
    "        self.y_train = th.Tensor(nData,3,3).type(dtype) # Target output\n",
    "        \n",
    "        Invar=Invariant()\n",
    "        self.x_train=Invar.getInvariants(S,R)\n",
    "        self.t_train=Invar.getTensorFunctions(S,R)   \n",
    "        self.k_train=k\n",
    "        self.y_train=b_avg.view(-1,3,3)  ##[7096, 9]\n",
    "        \n",
    "    \n",
    "        self.turb_nn = TurbNN(D_in=5, H=200, D_out=10).double() #Construct neural network        \n",
    "        # Get invarients\n",
    "        \n",
    "        # Student's t-distribution: w ~ St(w | mu=0, lambda=shape/rate, nu=2*shape)\n",
    "        # See PRML by Bishop Page 103\n",
    "        self.prior_w_shape = 1.0\n",
    "        self.prior_w_rate = 0.025\n",
    "        # noise variance: beta ~ Gamma(beta | shape, rate)\n",
    "        self.prior_beta_shape = 100\n",
    "        self.prior_beta_rate = 2e-4\n",
    "        \n",
    "        \n",
    "        # Now set up parameters for the noise hyper-prior\n",
    "        #beta_size = (self.n_samples, 1)\n",
    "        beta_size=1\n",
    "        log_beta = np.log(np.random.gamma(self.prior_beta_shape,\n",
    "                        1. / self.prior_beta_rate, size=beta_size))\n",
    "        # Log of the additive output-wise noise (beta)\n",
    "      \n",
    "        self.turb_nn.log_beta = Parameter(th.Tensor(log_beta).type(dtype)) ## AA: have to see\n",
    "\n",
    "        # Network weights learning weight\n",
    "        lr = 1e-3 #Original 1e-05\n",
    "        # Output-wise noise learning weight\n",
    "        lr_noise=0.001\n",
    "     \n",
    "        # Pre-pend output-wise noise to model parameter list\n",
    "        parameters = [{'params': [self.turb_nn.log_beta], 'lr': lr_noise},  ## AA: have to see\n",
    "                  {'params': [p for n, p in self.turb_nn.named_parameters() if n!='log_beta']}]\n",
    "        # ADAM optimizer (minor weight decay)\n",
    "        self.optim = th.optim.Adam(parameters, lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
    "        # Decay learning weight on plateau, can adjust these parameters depending on data\n",
    "        self.scheduler = ReduceLROnPlateau(self.optim, mode='min', factor=0.75, patience=3,\n",
    "            verbose=True, threshold=0.05, threshold_mode='rel', cooldown=5, min_lr=0, eps=1e-07)\n",
    "\n",
    "    def forward(self, input, t_data):\n",
    "        \"\"\"\n",
    "        Computes all the `n_samples` NN output\n",
    "        Args: \n",
    "            input (Tensor): [nx5] tensor of input invariants\n",
    "            t_data (Tensor): [nx10x3x3] tensor of linear independent tensore basis functions\n",
    "        Return: out (Tensor): [nx3x3] tensor of predicted scaled anisotropic terms\n",
    "        \"\"\"\n",
    "        out_size = (3, 3)\n",
    "        output = Variable(th.Tensor(input.size(0), *out_size).type(dtype))\n",
    "        #output=self.turb_nn.forward(input)\n",
    "        g_pred = self.turb_nn.forward(input)\n",
    "        g_pred0 = th.unsqueeze(th.unsqueeze(g_pred, 2), 3)  ## AA: have to see, also is the n from SVGD or the no cells\n",
    "        output = th.sum(g_pred0*t_data,1)\n",
    "        return output\n",
    "      \n",
    "    def BayesianPosterior(self,output,target):\n",
    "        \"\"\"\n",
    "        Computer posterior value and its gradients\n",
    "        Args:\n",
    "            output: [Nx3x3] Forward pass of the Invarient Neural Network (b_pred)\n",
    "            Target: [Nx3x3] LES b values\n",
    "        Return:\n",
    "            posterior: (scalar). Value of the posterior\n",
    "            grads:list containing grad of each parameter set {beta, w_1,w_2...}\n",
    "        \"\"\"\n",
    "        log_likelihood= (-0.5 * self.turb_nn.log_beta.exp()              ## N_total/N_batch can be added at the start to accomodate for mini batching. The wieght of the likelihood needs to be adjusted as per the mini batch\n",
    "                                * (target - output).pow(2).sum()\n",
    "                                + 0.5 * target.numel()\n",
    "                                * self.turb_nn.log_beta)\n",
    "         # Log Gaussian weight prior\n",
    "            # See Eq. 17 in paper\n",
    "        prior_ws = Variable(th.Tensor([0]).type(dtype))\n",
    "        for param in self.turb_nn.parameters():\n",
    "            prior_ws += th.log1p(0.5 / self.prior_w_rate * param.pow(2)).sum()\n",
    "        prior_ws *= -(self.prior_w_shape + 0.5)\n",
    "\n",
    "        # Log Gamma Output-wise noise prior\n",
    "        # See Eq. 20 in paper\n",
    "        prior_log_beta = (self.prior_beta_shape * self.turb_nn.log_beta \\\n",
    "                              - self.turb_nn.log_beta.exp() * self.prior_beta_rate).sum()\n",
    "        posterior=log_likelihood + prior_ws + prior_log_beta\n",
    "        #posterior.backward()\n",
    "        #grads = []\n",
    "        #for param in self.turb_nn.parameters():\n",
    "        #    grads.append(param.grad)\n",
    "        #grads = th.cat(grads)\n",
    "        #return posterior, grads\n",
    "        return posterior, log_likelihood.data.item()\n",
    "    \n",
    "    def loh_h_MCMC(self):\n",
    "        b_pred=self.forward(self.x_train,self.t_train)\n",
    "        posterior=self.BayesianPosterior(b_pred,self.y_train)\n",
    "        posterior.backward()\n",
    "        grads = []\n",
    "        for param in self.turb_nn.parameters():\n",
    "            grads.append(param.grad)\n",
    "        return posterior,grads\n",
    "        \n",
    "    \n",
    "    def MAP_train(self, n_epoch=200, gpu=True):\n",
    "        \"\"\"\n",
    "        for epoch in range(n_epoch):\n",
    "            training_loss=0\n",
    "            training_MSE=0\n",
    "            N_total=self.x_train.size()[0]\n",
    "            perm=np.random.permutation(N_total)\n",
    "            \n",
    "            batch_size=N_total/100\n",
    "            for it in range(0,N_total,batch_size)\n",
    "                idx=perm[np.arange(it,it+batch_size)]\n",
    "                x_batch=self.x_train[idx,:,:]\n",
    "                y_batch=self.y_train[idx,:,:]\n",
    "                t_batch=self.t_train[idx,:,:]\n",
    "                self.optim.zero_grad()\n",
    "                b_pred=self.forward(x_batch,t_batch)\n",
    "                posterior,log_likelihood=self.BayesianPosterior(b_pred,y_batch)\n",
    "                J=-posterior\n",
    "                J.backward()\n",
    "                self.optim.step()\n",
    "            \n",
    "                training_loss += posterior.data.item()\n",
    "                # Scaled MSE\n",
    "                training_MSE += (1/b_pred.size()[0])((self.y_train - b_pred) ** 2).sum().data.item()\n",
    "                print(\"===> Epoch: {}, Current loss: {:.6f} Log Beta: {:.6f} Scaled-MSE: {:.6f}\".format(\n",
    "                    epoch + 1, training_loss, self.turb_nn.log_beta.data.item(), training_MSE))\n",
    "        \"\"\"\n",
    "        for epoch in range(n_epoch):\n",
    "                self.optim.zero_grad()\n",
    "                b_pred=self.forward(self.x_train,self.t_train)\n",
    "                posterior,log_likelihood=self.BayesianPosterior(b_pred,self.y_train)\n",
    "                J=-posterior\n",
    "                J.backward()\n",
    "                self.optim.step()\n",
    "                \n",
    "                training_loss = posterior.data.item()\n",
    "                # Scaled MSE\n",
    "                loss=th.nn.MSELoss()\n",
    "                #training_MSE = loss(self.y_train,b_pred)/(self.y_train**2).mean() #||b_hat-b_LES||^2/||b_LES||^2\n",
    "                training_MSE = (((self.y_train-b_pred)**2).mean())/(self.y_train**2).mean()\n",
    "                if epoch%10==0:\n",
    "                    print(\"===> Epoch: {}, Posterior: {:.6f} Log Beta: {:.6f} Scaled-MSE: {:.6f}\".format(\n",
    "                    epoch + 1, posterior.data.item(), self.turb_nn.log_beta.data.item(), training_MSE))\n",
    "    \n",
    "    def predict(self,ransS,ransR):\n",
    "        \"\"\"\n",
    "        After training does a forward pass \n",
    "        Args:\n",
    "            ransS (Tensor): [nCellx3x3] Baseline RANS rate-of-strain tensor\n",
    "            ransR (Tensor): [nCellx3x3] Baseline RANS rotation tensor\n",
    "        return:\n",
    "            b_pred: The predicted value of the b    \n",
    "        \"\"\"\n",
    "        temp=Invariant()\n",
    "        x_train=temp.getInvariants(ransS,ransR)\n",
    "        t_train=temp.getTensorFunctions(ransS,ransR)\n",
    "        \n",
    "        b_pred=self.forward(x_train,t_train)\n",
    "        # error computation, need to input DNS/LES only for predicted domain. full LES imported for the time being\n",
    "        loss=th.nn.MSELoss()\n",
    "        error_pred=loss(self.y_train,b_pred)\n",
    "        \n",
    "        return b_pred,error_pred\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "1. Pre processimng script in each flow so convert OpenFOam DATA in tensors. then these can just be loaded with th.load. datamanager and torchreader are just loading these .th files for S, R, K and RS term and these are called imn FOAMsvgd file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dtype = th.DoubleTensor\n",
    "S=th.load('training-data/periodic-hills/RANS/90/S-torch.th')\n",
    "R=th.load('training-data/periodic-hills/RANS/90/R-torch.th')\n",
    "k0=th.load('training-data/periodic-hills/RANS/90/k-torch.th')\n",
    "RS_avg=th.load('training-data/periodic-hills/LES/1000/UPrime2Mean-torch.th')\n",
    "RS_avg=RS_avg.view(RS_avg.size()[0],3,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = k0.unsqueeze(0).unsqueeze(0).expand(3,3,k0.size()[0]) ## Two unsqueeze made it from 7096 to 1,1,7096\n",
    "k = k.permute(2, 0, 1) ## makes it 7096x3x3x from 3x3x7096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_avg = RS_avg/2*k - (2.0/3.0)*th.eye(3).type(dtype)  ## torch.Size([7096, 3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nData=S.size()[0]\n",
    "x_train = th.Tensor(nData,5).type(dtype) # Invariant inputs\n",
    "t_train = th.Tensor(nData,10,3,3).type(dtype) # Tensor basis\n",
    "k_train = th.Tensor(nData).type(dtype) # RANS TKE\n",
    "y_train = th.Tensor(nData,9).type(dtype) # Target output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Invar=Invariant()\n",
    "x_train=Invar.getInvariants(S,R)\n",
    "t_train=Invar.getTensorFunctions(S,R)   \n",
    "k_train=k0\n",
    "y_train=b_avg.view(-1,3,3)  ##[7096, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InvaNN=InvarientNN()\n",
    "\n",
    "out=InvaNN.forward(x_train,t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b=InvaNN.BayesianPosterior(out,y_train.view(-1,3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RS_pred=(out+(2.0/3.0)*th.eye(3).type(dtype))*k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trace(b_avg[100,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in InvaNN.turb_nn.parameters():\n",
    "    print(param.size())\n",
    "    #print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[1].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP point\n",
    "The trained $\\{\\theta,\\beta\\}$ can be found used ADAM optimiser to start the MCMC chain with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Invar=InvarientNN(S,R,k0,b_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Epoch: 1, Posterior: -3051993426.128128 Log Beta: 13.108161 Scaled-MSE: 1.317538\n",
      "===> Epoch: 11, Posterior: -2428350007.089494 Log Beta: 13.098256 Scaled-MSE: 1.058812\n",
      "===> Epoch: 21, Posterior: -2322753137.524028 Log Beta: 13.088683 Scaled-MSE: 1.022548\n",
      "===> Epoch: 31, Posterior: -2268604944.374339 Log Beta: 13.079276 Scaled-MSE: 1.008163\n",
      "===> Epoch: 41, Posterior: -2236003232.089691 Log Beta: 13.069919 Scaled-MSE: 1.003021\n",
      "===> Epoch: 51, Posterior: -2210587503.146106 Log Beta: 13.060570 Scaled-MSE: 1.000935\n",
      "===> Epoch: 61, Posterior: -2189861552.440977 Log Beta: 13.051216 Scaled-MSE: 1.000870\n",
      "===> Epoch: 71, Posterior: -2168918673.682462 Log Beta: 13.041854 Scaled-MSE: 1.000623\n",
      "===> Epoch: 81, Posterior: -2148520048.446677 Log Beta: 13.032492 Scaled-MSE: 1.000538\n",
      "===> Epoch: 91, Posterior: -2128382346.648794 Log Beta: 13.023137 Scaled-MSE: 1.000479\n",
      "===> Epoch: 101, Posterior: -2108487690.208157 Log Beta: 13.013794 Scaled-MSE: 1.000433\n",
      "===> Epoch: 111, Posterior: -2088850190.538400 Log Beta: 13.004470 Scaled-MSE: 1.000404\n",
      "===> Epoch: 121, Posterior: -2069455810.465829 Log Beta: 12.995168 Scaled-MSE: 1.000382\n",
      "===> Epoch: 131, Posterior: -2050297694.608176 Log Beta: 12.985890 Scaled-MSE: 1.000363\n",
      "===> Epoch: 141, Posterior: -2031374201.306605 Log Beta: 12.976640 Scaled-MSE: 1.000345\n",
      "===> Epoch: 151, Posterior: -2012687045.716700 Log Beta: 12.967419 Scaled-MSE: 1.000329\n",
      "===> Epoch: 161, Posterior: -1994234333.185404 Log Beta: 12.958228 Scaled-MSE: 1.000315\n",
      "===> Epoch: 171, Posterior: -1976008691.005642 Log Beta: 12.949069 Scaled-MSE: 1.000298\n",
      "===> Epoch: 181, Posterior: -1958010370.034652 Log Beta: 12.939942 Scaled-MSE: 1.000280\n",
      "===> Epoch: 191, Posterior: -1940237054.480261 Log Beta: 12.930848 Scaled-MSE: 1.000260\n"
     ]
    }
   ],
   "source": [
    "Invar.MAP_train()   ##For a sucessfult training posterior should increase ()absolute value doesnt matter) and MSe should decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_pred,error_prediction=Invar.predict(S,R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Invar.y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=Invar.forward(Invar.x_train,Invar.t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b=Invar.loh_h_MCMC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chk=((Invar.y_train-out)**2).sum(2).sum(1).sum(0).data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chk.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Machine Learning based approach for investigating Reynolds stress discrepancy based on D\n",
    "Invar.turb_nn.named_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,p in Invar.turb_nn.named_parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,100,10):\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
